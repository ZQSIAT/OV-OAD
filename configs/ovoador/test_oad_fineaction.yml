_base_: '../default.yml'
model_name: 'ovoador' # display name in the logger
use_nvideos: -1  # total: 14950, -1 means use all videos 
# output: /mnt/cache/heyinan/00_zqs/data/exps/debugs
# output: /mnt/cache/heyinan/00_zqs/data/exps/anet_enc32_3layers_lsdecoder0.1_multilabel_diceloss1.0
# output: /mnt/cache/heyinan/00_zqs/data/exps/anet_enc32_3layers_lsxattn0.1_multilabel_diceloss1.0_focalloss5.0
# output: /mnt/cache/heyinan/00_zqs/data/exps/anet_enc32_3layers_no-mask_lsxattn0.1_multilabel_diceloss1.0
# output: /mnt/cache/heyinan/00_zqs/data/exps/2024_0809_recurrent_anet_enc32_3layers_lsxattn0.1_multilabel_diceloss1.0
# output: /mnt/cache/heyinan/00_zqs/data/exps/anet_enc32_3layers_k600-timesformer-adapter_lsxattn0.1_multilabel_diceloss1.0
# output: /mnt/cache/heyinan/00_zqs/data/exps/anet_enc32_3layers_l_detach_sxattn0.1_multilabel_maskloss
# output: /mnt/cache/heyinan/00_zqs/data/exps/anet_enc32_3-layers_txt.enc-unfixed_lsxattn-0.1_multilabel-maskloss # not only verb template
# output: /mnt/cache/heyinan/00_zqs/data/exps/anet_enc32_3-layers_lsxattn-0.1_multilabel # not only verb template
# output: /mnt/cache/heyinan/00_zqs/data/exps/anet_enc32_3-layers_depth-6.4.2_lsxattn-0.1_multilabel-maskloss # not only verb template
# output: /mnt/cache/heyinan/00_zqs/data/exps/anet_enc32_3-layers_txt_unfixed_only_mask_short_lsxattn-0.1_multilabel-maskloss # not only verb template
# output: /mnt/cache/heyinan/00_zqs/data/exps/anet_enc32_3-layers_no_adapter_lsxattn-0.1_multilabel-maskloss # not only verb template
# output: /mnt/cache/heyinan/00_zqs/data/exps/anet_enc32_3-layers_l_dw_conv1d_compress_sxattn-0.1_multilabel-maskloss
# output: /mnt/cache/heyinan/00_zqs/data/exps/anet_enc32_3-layers_txt_unfixed_l_detach_sxattn-0.1_multilabel-maskloss
# output: /mnt/cache/heyinan/00_zqs/data/exps/anet_enc32_3-layers_txt_unfixed_ls_detach_xattn-0.1_multilabel-diceloss  # diceloss = maskloss
# output: /mnt/cache/heyinan/00_zqs/data/exps/anet_enc32_3-layers_switch-off-groupvit_lsxattn-0.1_multilabel-diceloss
output: /mnt/cache/heyinan/00_zqs/data/exps/2024_0811_eval_ovoad_fineaction
device: cuda
debug: False
print_freq: 100
single_eval: False

data:
  with_dc: False
  train: 
    root_dir: 'phdd:s3://anet_extc_feats_4fps/CLIP_ViT_B_16_768/'
    meta_file: '/mnt/petrelfs/heyinan/00_zqs/code/ovoad/extract_features/data_list/new_data_info.json'
    anno_file: '/mnt/petrelfs/heyinan/00_zqs/data/anet/anno'
    read_from: petrel
    use_nvideos: ${use_nvideos} 
    nonzero: 0  
    enc_steps: 32 # 128
    dec_steps: 8
    caption_pick: 'txt_cat'
    use_dali: True
    batch_size: 256  # 256
    input_size: 224
    test_resize: 256
    verb_aug: false
    verb_filter: false # 剔除不包含verb的caption, 没用
    use_saliency: ${model.use_saliency}
    use_enc_feat: ${model.use_enc_feat}
    saliency_steps: ${model.saliency_steps} # number frame for saliency branch, default is 1
    image_reader:
          type: pil
    sampler:
          type: distributed_epoch
    transforms:
          type: STANDARD
    use_ranked: False
    
    use_entity: ${model.use_entityloss} 
    mask_type: verb  # dobj / verb 预测动词或动宾短语
    use_distilbert: True

  text_aug:
    max_seq_len: 77
    multi_label: 3 # we use multi-label contrastive 
    word_type: 'verb'

  val:
      type: clip
      read_from: petrel
      use_dali: True
      batch_size: 64
      num_workers: 4
      pin_memory: False
      input_size: 224
      test_resize: 256
      
      root_dir: '/mnt/cache/share/images/val/'
      meta_file: 'imagenet_info/val.json'
      # you can change it to imagenet_info relative path, file already in gitlab
      image_reader:
          type: pil
      sampler:
          type: distributed
      transforms:
          type: ONECROP
      evaluator:
          type: imagenet
          kwargs:
              topk: [1, 5]
      label_texts_ensemble: 'prompt1'
          
model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 768  # small : 384; base: 768
    # num_heads: [8, 8]  # small : [6, 6] 
    # embed_factors: [1, 1]
    # depths: [6, 6] # [6, 6]
    # num_group_tokens: [8, 0]
    # num_output_groups: [2]

    num_heads: [8, 8, 8]  # small : [6, 6] 
    embed_factors: [1, 1, 1]
    depths: [4, 6, 2] # [4, 6, 2]
    num_group_tokens: [0, 8, 0]
    num_output_groups: [0, 2]

    # num_heads: [8, 8, 8]  # small : [6, 6] 
    # embed_factors: [1, 1, 1]
    # depths: [6, 4, 2] # [4, 6, 2]
    # num_group_tokens: [0, 8, 0]
    # num_output_groups: [0, 2]

    drop_rate: 0.1 # 0.1 
    attn_drop_rate: 0.1 # 0.1 
    drop_path_rate: 0.1 # 0.1 
    no_patch_embed: true 
    enc_steps: ${data.train.enc_steps} 
    dec_steps: ${data.train.dec_steps} 
    patch_norm: false

    use_enc_feat: ${model.use_enc_feat} 
    enc_feat_lsxattn: ${model.enc_feat_lsxattn} 
    long_term_detach: false  # whether detach the long term memory attn maps, default: false 33.7, true 33.8
    switch_off_layer0: false # whether switch off the long short-term branch and groupvit ? default: false

    mask_attn_layers: [0]  # default: [0] , [0, 1]
    only_mask_short: false  # whether only mask the short term frames, default: false
    long_term_compress: null  # enc_feat_lsxattn & use_enc_feat must be true, null , dw_conv1d, conv1d, causal_conv1d: default: null

    pre_proj: "identity" # use one layer projection before the groupvit: identity, linear, adapter_serial, adapter_parallel 
    imgnet_pretrained: null  # null, timesformer
    fixed: false
    # imgnet_pretrained_checkpoint: "/mnt/cache/heyinan/00_zqs/data/pretrained_models/TimeSformer_divST_8_224_SSv2.pyth"
    imgnet_pretrained_checkpoint: '/mnt/cache/heyinan/00_zqs/data/pretrained_models/TimeSformer_divST_8x32_224_K600.pyth'
    # imgnet_pretrained_checkpoint: "/mnt/cache/heyinan/00_zqs/data/pretrained_models/dinov2_vitb14_pretrain.pth"
    # imgnet_pretrained_checkpoint: "/mnt/cache/heyinan/00_zqs/data/pretrained_models/dino_vitbase16_pretrain.pth"
    # imgnet_pretrained_checkpoint: "/mnt/cache/heyinan/00_zqs/data/pretrained_models/TimeSformer_divST_32x32_224_HowTo100M.pyth"
    # tune_config:
    #   ffn_adapt: true  # 'whether activate AdaptFormer'
    #   ffn_option: "serial" # two adapters after MHS and MLP "serial" or "parallel"
    #   ffn_adapter_layernorm_option: "none"
    #   ffn_adapter_init_option: "lora"
    #   ffn_adapter_scalar: "0.1"
    #   ffn_num: 128 # why?
    #   d_model: ${model.img_encoder.embed_dim}

  text_encoder:
    type: CLIPTransformer
    context_length: 77
    width: 512
    layers: 12
    vocab_size: 49408
    pretrained: true
    fixed: false
    ffn_adapt: true  # 'whether activate AdaptFormer'
    ffn_option: "parallel"
    ffn_adapter_layernorm_option: "none"
    ffn_adapter_init_option: "lora"
    ffn_adapter_scalar: "0.1"
    ffn_num: 64
  contrast_temperature: 0.07
  proj_num_layers: 2 # when -1 text projection must be used!! as 2 commit the text projection !!
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}

  use_entityloss: false
  entity_weight: 1.0

  use_diceloss: true
  diceloss_weight: 1.0

  use_focalloss: false
  focalloss_weight: 5.0

  use_saliency: false 
  saliency_steps: 1 # number frame for saliency branch, default is 1
  saliency_weight: 0.1
  
  use_enc_feat: true 
  enc_feat_lsxattn: true
  enc_feat_weight: 0.1


  
train:
  epochs: 30
  # base_lr: 1.6e-5
  # warmup_lr: 6.4e-7 
  # warmup_epochs: 10
  # min_lr: 6.4e-6
  base_lr: 6.4e-5
  warmup_lr: 1.6e-6 
  warmup_epochs: 2
  min_lr: 1.6e-5
  clip_grad: 5.0
checkpoint:
  auto_resume: false
  resume: null
  save_freq: 50
evaluate:
  eval_freq: 1
  oad:
    enc_steps: ${data.train.enc_steps}
    eval_type: "means" # set saliency probs
    use_saliency: ${model.use_saliency}
    use_enc_feat: ${model.use_enc_feat}
    single_eval: ${single_eval}
    datasets: [
      # 'thomus', 
      'fineaction',
      # 'tvseries',
      # 'epic',
      # 'anet',
      ]
    cfg: [
      # 'oad/configs/_base_/datasets/thomus14.py',
      'oad/configs/_base_/datasets/fineaction.py',
      # 'oad/configs/_base_/datasets/tvseries.py',
      # 'oad/configs/_base_/datasets/epic_verb.py', # class_type="verb_perframe"
      # 'oad/configs/_base_/datasets/anet.py',
      ]
    template: simple_kinectics  # full_kinectics, subset_kinectics, simple_imagenet, etc

